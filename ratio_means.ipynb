{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from statistical_methods_library.imputation import ratio_of_means, impute\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import DecimalType, LongType, StringType\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[2]\") \\\n",
    "      .appName(\"ratio_means\") \\\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date string\n",
    "#identifie string\n",
    "#group string\n",
    "#question decimal\n",
    "#other decimal\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"identifier\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"group\", StringType()),\n",
    "    StructField(\"question\", DecimalType(15,6)),\n",
    "    StructField(\"other\", DecimalType(15,6))\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"df.csv\",header=True, schema=schema\n",
    ")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"reference_col\": reference_col,\n",
    "    \"period_col\": period_col,\n",
    "    \"grouping_col\": grouping_col,\n",
    "    \"target_col\": target_col,\n",
    "    \"auxiliary_col\": auxiliary_col,\n",
    "    \"output_col\": output_col,\n",
    "    \"marker_col\": marker_col,\n",
    "    \"forward_backward_ratio_calculator\": ratio_of_means,\n",
    "}\n",
    "\n",
    "df2 = impute(input_df=df,**params)\n",
    "\n",
    "df2.toPandas().to_csv(\"df2.csv\")\n",
    "\n",
    "\n",
    "#df2.write.csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [backward#10416, construction#10591, count_backward#10486L, count_construction#10626L, count_forward#10451L, default_backward#10556, default_construction#10661, default_forward#10521, forward#10381, grouping#10018 AS group#11802, marker#11696, output#11695, period#10017 AS date#11805, ref#10016 AS identifier#11806]\n",
      "   +- BroadcastNestedLoopJoin BuildRight, Inner, (prior_period#10125 < period#10017)\n",
      "      :- Project [ref#10016, period#10017, grouping#10018, forward#10381, backward#10416, count_forward#10451L, count_backward#10486L, default_forward#10521, default_backward#10556, construction#10591, count_construction#10626L, default_construction#10661, output#11695, marker#11696]\n",
      "      :  +- Filter isnotnull(period#10017)\n",
      "      :     +- Scan ExistingRDD[ref#10016,period#10017,grouping#10018,aux#10020,previous_period#10108,next_period#10116,forward#10381,backward#10416,count_forward#10451L,count_backward#10486L,default_forward#10521,default_backward#10556,construction#10591,count_construction#10626L,default_construction#10661,output#11695,marker#11696]\n",
      "      +- BroadcastExchange IdentityBroadcastMode, [plan_id=21920]\n",
      "         +- Filter isnotnull(prior_period#10125)\n",
      "            +- Scan ExistingRDD[prior_period#10125]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
